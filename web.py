import logging
import os

import gradio as gr
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_litellm import ChatLiteLLM
from langchain_neo4j import Neo4jVector
from rich.logging import RichHandler

# è®¾ç½®æ—¥å¿—è®°å½•
logger = logging.getLogger("chat")
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    datefmt="[%X]",
    handlers=[RichHandler(rich_tracebacks=True)],
)

# ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
load_dotenv()

# åˆå§‹åŒ– LLM
llm = ChatLiteLLM(
    model="deepseek/deepseek-chat",
    temperature=0.7,
    api_key=os.getenv("API_KEY", "enter_your_api_key_in_.env"),
)

# åˆå§‹åŒ– Embeddings å’Œ Neo4j
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")

neo4j_vector = Neo4jVector.from_existing_graph(
    embedding=embeddings,
    node_label="Document",
    embedding_node_property="embedding",
    text_node_properties=["name", "description"],
    url=os.getenv("NEO4J_URL", "enter_your_neo4j_url_in_.env"),
    username=os.getenv("NEO4J_USER", "enter_your_neo4j_username_in_.env"),
    password=os.getenv("NEO4J_PASSWORD", "enter_your_neo4j_password_in_.env"),
)


def retrieve_from_neo4j(query: str, top_k: int = 5):
    """ä» Neo4j æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
    try:
        logger.info(f"ğŸ” æ­£åœ¨æ£€ç´¢ä¸é—®é¢˜ç›¸å…³çš„çŸ¥è¯†: {query}")
        results = neo4j_vector.similarity_search(query, k=top_k)
        if not results:
            return []

        contexts = [doc.page_content for doc in results]
        logger.info(f"âœ… æ£€ç´¢åˆ° {len(contexts)} æ¡ç›¸å…³çŸ¥è¯†")
        logger.info(f"ğŸ”— ç›¸å…³çŸ¥è¯†å†…å®¹: {contexts}")
        return contexts
    except Exception as e:
        logger.error(f"âŒ Neo4j æ£€ç´¢å¤±è´¥: {e}")
        return []


def rag_chat(query: str):
    """åŸºäº RAG çš„é—®ç­”æµç¨‹"""
    # æ£€ç´¢ä¸Šä¸‹æ–‡
    context_list = retrieve_from_neo4j(query)
    if not context_list:
        return "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ï¼Œè¯·æ¢ä¸ªé—®é¢˜è¯•è¯•ï½"

    # æ‹¼æ¥ä¸Šä¸‹æ–‡
    context_str = "\n".join(f"- {c}" for c in context_list)

    # æ„å»ºæç¤ºè¯
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ä¸°å¯Œçš„AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†å›ç­”é—®é¢˜ã€‚

    èƒŒæ™¯çŸ¥è¯†:
    {context_str}

    é—®é¢˜:
    {query}

    è¯·ç”¨ç®€æ´ã€å‡†ç¡®çš„è¯­è¨€å›ç­”:
    """

    # è°ƒç”¨ LLM
    try:
        logger.info("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
        response = llm.invoke(prompt)
        return str(response).strip()
    except Exception as e:
        logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        return "ç”Ÿæˆå›ç­”å¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"


# Gradio ç•Œé¢éƒ¨åˆ†
def gradio_interface(user_query):
    return rag_chat(user_query)


with gr.Blocks(title="RAG é—®ç­”ç³»ç»Ÿ") as demo:
    gr.Markdown("# ğŸ’¬ RAG é—®ç­”ç³»ç»Ÿ")
    with gr.Row():
        with gr.Column():
            user_input = gr.Textbox(
                label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šè„±æ•æ°‘èˆªæ˜¯ä»€ä¹ˆï¼Ÿ"
            )
            submit_button = gr.Button("æäº¤")
        with gr.Column():
            answer_output = gr.Textbox(
                label="ğŸ¤– å›ç­”", placeholder="AI å›ç­”å°†åœ¨è¿™é‡Œæ˜¾ç¤º", lines=5
            )

    submit_button.click(fn=gradio_interface, inputs=user_input, outputs=answer_output)

# å¯åŠ¨ Gradio
if __name__ == "__main__":
    demo.launch()
