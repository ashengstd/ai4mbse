import asyncio
import json
import logging
from typing import Any, Coroutine

from langchain.prompts import PromptTemplate
from langchain_litellm import ChatLiteLLM
from rich.logging import RichHandler

from chat.template import triple_prompt_template

logger = logging.getLogger("triple_extractor")
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    datefmt="[%X]",
    handlers=[RichHandler(rich_tracebacks=True, show_time=False, markup=True)],
)


# --- åŠ è½½ txt æ–‡ä»¶å¹¶åˆ†æ®µ ---
def split_paragraphs(content: str) -> list[str]:
    paragraphs = [p.strip() for p in content.split("\n") if p.strip()]
    return paragraphs


async def extract_triples_from_paragraphs(
    llm: ChatLiteLLM,
    p1: str,
    p2: str,
    p3: str,
    p4: str,
    logger: logging.Logger,
) -> str:
    """
    ä½¿ç”¨ LLM ä»å››ä¸ªæ®µè½ä¸­æå–ä¸‰å…ƒç»„ã€‚
    Args:
        p1, p2, p3, p4 (str): å››ä¸ªæ®µè½æ–‡æœ¬
    Returns:
        str: LLM è¿”å›çš„ JSON æ ¼å¼å­—ç¬¦ä¸²
    """
    prompt = PromptTemplate(
        input_variables=["p1", "p2", "p3", "p4"], template=triple_prompt_template
    ).format(p1=p1, p2=p2, p3=p3, p4=p4)
    logger.info(f"ğŸ§  æ­£åœ¨å¤„ç†æ®µè½ï¼š\n{p1}\n{p2}\n{p3}\n{p4}")
    response = llm.invoke(prompt)
    if not response.content:
        logger.error("âŒ LLM å“åº”å†…å®¹ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®æˆ–è¾“å…¥æ®µè½ã€‚")
        return ""
    return str(response.content)


# --- ä¸»æµç¨‹ï¼ˆä¿®æ”¹æ»‘åŠ¨çª—å£ä¸º4æ®µï¼Œæ­¥é•¿3ï¼‰ ---
async def extract_requirement_triples(
    llm: ChatLiteLLM,
    content: str,
    window_size=4,
    step=3,
) -> dict:
    """
    ä»è¾“å…¥æ–‡æœ¬ä¸­æå–éœ€æ±‚ç›¸å…³çš„ä¸‰å…ƒç»„ï¼Œå¹¶è¿”å› JSON ã€‚
    Args:
        content (str): è¾“å…¥æ–‡æœ¬æ–‡ä»¶å†…å®¹
        window_size (int): çª—å£å¤§å°ï¼Œé»˜è®¤ä¸º4æ®µ
        step (int): æ­¥é•¿ï¼Œé»˜è®¤ä¸º3æ®µ
    """
    paragraphs = split_paragraphs(content)
    output_triples = []

    tasks: list[Coroutine[Any, Any, str]] = []
    total_windows = (len(paragraphs) - window_size) // step + 1

    for i in range(total_windows):
        start_idx = i * step
        window_paragraphs = paragraphs[start_idx : start_idx + window_size]

        # ç¡®ä¿æœ‰4æ®µ
        if len(window_paragraphs) < window_size:
            break

        tasks.append(
            extract_triples_from_paragraphs(
                llm=llm,
                p1=window_paragraphs[0],
                p2=window_paragraphs[1],
                p3=window_paragraphs[2],
                p4=window_paragraphs[3],
                logger=logger,
            )
        )

    results = await asyncio.gather(*tasks)

    for i, result in enumerate(results):
        try:
            result_json = json.loads(result)
        except json.JSONDecodeError:
            logger.error(f"âŒ JSON è§£æå¤±è´¥ï¼š{result}")
            continue

        triples = result_json.get("triples", [])  # æ³¨æ„è¿™é‡Œç”¨ "triples"
        output_triples.extend(triples)
        logger.info(f"âœ… å·²å¤„ç†çª—å£ {i + 1}/{total_windows}")

    logger.info(f"ğŸ‰ æå–å®Œæˆï¼Œå…±æå–ä¸‰å…ƒç»„æ•°: {len(output_triples)}")
    return {"triples": output_triples}


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_content = (
        "ç³»ç»Ÿåº”è¯¥å…è®¸ç”¨æˆ·ç™»å½•ã€‚\n"
        "ç”¨æˆ·å¯ä»¥ä½¿ç”¨ç”¨æˆ·åå’Œå¯†ç è¿›è¡Œè®¤è¯ã€‚\n"
        "ç³»ç»Ÿå…·æœ‰ä¸€ç³»åˆ—å®‰å…¨æªæ–½ã€‚\n"
        "ç³»ç»Ÿä¿è¯ç”¨æˆ·çš„ç‹¬ä¸€ã€‚"
    )
    llm = ChatLiteLLM(model="deepseek/deepseek-chat")
    result = asyncio.run(extract_requirement_triples(llm=llm, content=test_content))
    print(json.dumps(result, ensure_ascii=False, indent=2))
